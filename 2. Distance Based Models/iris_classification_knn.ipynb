{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbORlHDOEYTE"
      },
      "source": [
        "## Classifiers comparison: decision trees and k-nearest neighbors on the dataset Iris\n",
        "\n",
        "\n",
        "In the following program we compare the prediction results obtained by decision trees and k-nearest neighbors on the dataset Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iFRSTXNDEYTG"
      },
      "source": [
        "The following cell shows the program training a decision tree and its results in preciction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7fYOeG3EYTH",
        "outputId": "94c06325-bd74-47a6-92b7-8e6158afddd2"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree \n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score # will be used to separate training and test\n",
        "import itertools\n",
        "iris = load_iris()\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})\n",
        "clf = clf.fit(iris.data, iris.target)\n",
        "scores = cross_val_score(clf, iris.data, iris.target, cv=5) # score will be the accuracy\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Stp1gpEYTP"
      },
      "source": [
        "The following cell shows the training of k-nearest neighbors and its prediction results.\n",
        "Here we use a uniform weighting setting (weights='uniform'): any neighbors weights the same in the majority voting aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZAavM5qEYTQ",
        "outputId": "75239f26-c991-4aa5-af30-4e1e11e60e34"
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors\n",
        "n_neighbors = 11\n",
        "clf_knn = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n",
        "clf_knn = clf_knn.fit(iris.data, iris.target)\n",
        "scores = cross_val_score(clf_knn, iris.data, iris.target, cv=5) # score will be the accuracy\n",
        "print(scores)\n",
        "# shows the model predictions  \n",
        "for i in range(len(iris.target)):\n",
        "    print(iris.data[i,:])\n",
        "    print(iris.data[i,:].reshape(1,-1))\n",
        "    instance=iris.data[i,:].reshape(1,-1)\n",
        "    #print(clf_knn.predict(instance))\n",
        "    predicted=clf_knn.predict(instance)[0]\n",
        "    print(predicted)\n",
        "    if iris.target[i]==predicted:\n",
        "        print(str(i)+\" ok \"+str(iris.target_names[iris.target[i]]))\n",
        "    else:\n",
        "        print(str(i)+\" nok \"+\"true class: \"+str(iris.target_names[iris.target[i]])+\"; predicted: \"+str(iris.target_names[predicted]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVI8abkeEYTV"
      },
      "source": [
        "In the following cell we use a varying weighting setting (weights='distance'): any neighbors weights inversely with its distance to the test instance in the majority voting aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxC1EypXEYTV",
        "outputId": "04a3ea00-6045-4507-e3c8-98bc124b4a5b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "n_neighbors = 11\n",
        "clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "clf_knn2.fit(iris.data, iris.target)\n",
        "\n",
        "for i in range(len(iris.target)):\n",
        "    instance=(iris.data[i,:]).reshape(1, -1)\n",
        "    predicted2=clf_knn2.predict(instance)[0]\n",
        "    if iris.target[i]==predicted2:\n",
        "        print(str(i)+\" ok \"+str(iris.target_names[iris.target[i]]))\n",
        "    else:\n",
        "        print(str(i)+\" nok \"+\"true class: \"+str(iris.target_names[iris.target[i]])+\"; predicted: \"+str(iris.target_names[predicted]))\n",
        "print(\"Classification score of k-nn with distance weighting\")\n",
        "scores2 = cross_val_score(clf_knn2, iris.data, iris.target, cv=5,scoring='accuracy') # score will be the accuracy\n",
        "print(scores2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wpx4NW0EYTa"
      },
      "source": [
        "The following cell shows the tuning of the k-nn models with a varying value of k (number of nearest neighbors) and finds the best value of k (giving the maximum accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-jhdpzK2EYTa",
        "outputId": "6eadc375-90cb-46bf-d6eb-237b942ff42f",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from sklearn import neighbors\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "best_accuracy=0\n",
        "best_k=1\n",
        "A=np.zeros(len(y_train), dtype=np.float) # for storing accuracies\n",
        "for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "    clf_knn3 = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "    # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "    clf_knn3.fit(X_train, y_train)\n",
        "    index=n_neighbors-1\n",
        "    A[index]=clf_knn3.score(X_test, y_test)\n",
        "    if best_accuracy<clf_knn3.score(X_test, y_test):\n",
        "        best_accuracy=clf_knn3.score(X_test, y_test)\n",
        "        best_k=n_neighbors\n",
        "    print(\"k neighbors=\"+str(n_neighbors))\n",
        "    print(\"accuracy=\"+str(clf_knn3.score(X_test, y_test)))\n",
        "    \n",
        "print(\"\\n\")\n",
        "print(\"best k=\"+str(best_k))\n",
        "print(\"best accuracy=\"+str(best_accuracy))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "#plt.xticks(np.arange(1, len(y_train)+1, 8))\n",
        "plt.yticks(np.arange(0.0,1.0,0.01))\n",
        "\n",
        "plt.plot(np.arange(1,len(y_train)+1),A)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "p23LUav0EYTe"
      },
      "source": [
        "In the following cell we plot in the same plot two subplots with the diagrams on accuracy with the two kinds of weighting \n",
        "of the vote of the neighbours (uniform and with distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ewb11oFOEYTf",
        "outputId": "a64d135d-b84c-4051-f1b6-5cd248b5c70f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from sklearn import neighbors\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "i=0  #parameter in the control of the subplot to draw on'\n",
        "f,(ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "for weight_type in ['uniform','distance']:\n",
        "    print(\"weighting:\"+str(weight_type))\n",
        "    A=np.zeros(len(y_train), dtype=np.float) # for storing accuracies\n",
        "    best_accuracy=0\n",
        "    best_k=1\n",
        "    for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "        clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights=weight_type)\n",
        "        # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "        clf_knn2.fit(X_train, y_train)\n",
        "        index=n_neighbors-1\n",
        "        A[index]=clf_knn2.score(X_test, y_test)\n",
        "        if best_accuracy<clf_knn2.score(X_test, y_test):\n",
        "            best_accuracy=clf_knn2.score(X_test, y_test)\n",
        "            best_k=n_neighbors\n",
        "        print(\"k neighbors=\"+str(n_neighbors))\n",
        "        print(\"accuracy=\"+str(clf_knn2.score(X_test, y_test)))\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(\"best k=\"+str(best_k))\n",
        "    print(\"best accuracy=\"+str(best_accuracy))\n",
        "    if i==0:\n",
        "        ax1.plot(np.arange(1,len(y_train)+1),A)\n",
        "        ax1.set_title('weighting type:'+str(weight_type))\n",
        "    else:\n",
        "        ax2.plot(np.arange(1,len(y_train)+1),A)\n",
        "        ax2.set_title('weighting type:'+str(weight_type))\n",
        "    i=i+1\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "LXhkXpz0EYTj"
      },
      "source": [
        "In the following cell we plot (overlapping) in the same picture both the diagrams on accuracy with the two kinds of weighting \n",
        "of the vote of the neighbours (uniform and with distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7_nkikvMEYTk",
        "outputId": "e5fb0243-2767-4448-e9b5-7252c8bf1610"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from sklearn import neighbors\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Accuracy in k-nn with number of neighbors and types of weighting', fontsize=14, fontweight='bold')\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel('n. neighbors')\n",
        "ax.set_ylabel('accuracy')\n",
        "\n",
        "A=np.zeros((len(y_train),2), dtype=np.float) # 2 arrays for storing accuracies for each type of weigthing\n",
        "i=0  #parameter in the control of the different diagram (=matrix A column index)\n",
        "best_accuracy=0\n",
        "for weight_type in ['uniform','distance']:\n",
        "    print(\"\\n weighting:\"+str(weight_type))\n",
        "    best_accuracy=0\n",
        "    best_k=1\n",
        "    for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "        clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights=weight_type)\n",
        "        # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "        clf_knn2.fit(X_train, y_train)\n",
        "        index=n_neighbors-1 # computes the matrix row index\n",
        "        A[index,i]=clf_knn2.score(X_test, y_test)\n",
        "        if best_accuracy<clf_knn2.score(X_test, y_test):\n",
        "            best_accuracy=clf_knn2.score(X_test, y_test)\n",
        "            best_k=n_neighbors\n",
        "        print(\"k neighbors=\"+str(n_neighbors))\n",
        "        print(\"accuracy=\"+str(clf_knn2.score(X_test, y_test)))\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(\"best k=\"+str(best_k))\n",
        "    print(\"best accuracy=\"+str(best_accuracy))\n",
        "    plt.plot(np.arange(1,len(y_train)+1),A[:,i])\n",
        "    i=i+1\n",
        "plt.legend(['uniform', 'distance'], loc='lower left')  \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-aSX54vFEYTp"
      },
      "source": [
        "## 1) Show the scatter plot (in 2D, by choosing two of the four features) of the Iris data points, with a color determined by their class: red color for Setosa, blu for Versicolor, Green for Virginica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = iris.data\n",
        "y = iris.target\n",
        "palette = ['r', 'g', 'b']\n",
        "\n",
        "def plot_dataset(x, y, labels, colors):\n",
        "    plt.grid(True, alpha=0.4)\n",
        "    plt.xlabel(labels[0])\n",
        "    plt.ylabel(labels[1])\n",
        "    plt.scatter(x, y, c=colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o8cPLOdEYTq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 8))\n",
        "plt.suptitle(\"2-D Dataset Plots\")\n",
        "colors = [palette[val] for val in iris.target]\n",
        "\n",
        "combinations = itertools.combinations(range(len(iris.feature_names)), 2)\n",
        "for index, combination in enumerate(combinations):\n",
        "    plt.subplot(2, 3, index+1)\n",
        "\n",
        "    labels = iris.feature_names[combination[0]], iris.feature_names[combination[1]]\n",
        "    plot_dataset(X[:, combination[0]], X[:, combination[1]], labels, colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs9ogv4vEYTt"
      },
      "source": [
        "## 2) Show a similar scatter plot as in 1), but now the color of the dots is determined by the estimated class by the k-nn, with a chosen value of k of your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niu9K9kn43ZM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=300)\n",
        "n_neighbors = 7\n",
        "\n",
        "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "knn_classifier = knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "prediction = knn_classifier.predict(X)\n",
        "knn_classifier.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 8))\n",
        "plt.suptitle(\"2-D Dataset Plots\")\n",
        "colors = [palette[val] if prediction[i] == iris.target[i] else 'y' for i, val in enumerate(prediction)]\n",
        "\n",
        "combinations = itertools.combinations(range(len(iris.feature_names)), 2)\n",
        "for index, combination in enumerate(combinations):\n",
        "    plt.subplot(2, 3, index+1)\n",
        "\n",
        "    labels = iris.feature_names[combination[0]], iris.feature_names[combination[1]]\n",
        "    plot_dataset(X[:, combination[0]], X[:, combination[1]], labels, colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### I tried using worse parameters to check the resulting performance and plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "knn_classifier = neighbors.KNeighborsClassifier(80, weights='uniform')\n",
        "knn_classifier = knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "prediction = knn_classifier.predict(X)\n",
        "knn_classifier.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 8))\n",
        "plt.suptitle(\"2-D Dataset Plots\")\n",
        "colors = [palette[val] if prediction[i] == iris.target[i] else 'y' for i, val in enumerate(prediction)]\n",
        "\n",
        "combinations = itertools.combinations(range(len(iris.feature_names)), 2)\n",
        "for index, combination in enumerate(combinations):\n",
        "    plt.subplot(2, 3, index+1)\n",
        "\n",
        "    labels = iris.feature_names[combination[0]], iris.feature_names[combination[1]]\n",
        "    plot_dataset(X[:, combination[0]], X[:, combination[1]], labels, colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Run k-nn and use now as a distance function between two data points x and y, the function: distance(x,y)= 1- k(x,y) \n",
        "\n",
        "With k(x,y) the Radial Basis Function (a Gaussian-like function) with the parameter gamma that controls its spread. \n",
        "The parameter gamma must be tuned to the best value, according to the maximum accuracy of k-nn (similarly as we did previously for the parameter n_neighbors). In this case, choose a value of k=7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def radial_basis_function(x, y, gamma):\n",
        "    def k(x, y, gamma):\n",
        "        dist = np.linalg.norm(x - y)\n",
        "        return np.exp(-dist/2*gamma**2)\n",
        "\n",
        "    return 1 - k(x, y, gamma)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_sigma = 0\n",
        "\n",
        "# Tuning of sigma hyperparameter\n",
        "for sigma in np.linspace(0, 1, 101):\n",
        "    knn_classifier = neighbors.KNeighborsClassifier(n_neighbors, metric=radial_basis_function, metric_params={'gamma': sigma})\n",
        "    knn_classifier = knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "    accuracy = knn_classifier.score(X_test, y_test)\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_sigma = sigma\n",
        "\n",
        "print(best_accuracy, best_sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We use 0.01 as gamma value as obtained from the previous tuning step\n",
        "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors, metric=radial_basis_function, metric_params={'gamma': 0.01})\n",
        "knn_classifier = knn_classifier.fit(X_train, y_train)\n",
        "prediction = knn_classifier.predict(X)\n",
        "\n",
        "plt.figure(figsize=(25, 8))\n",
        "plt.suptitle(\"2-D Dataset Plots\")\n",
        "colors = [palette[val] if prediction[i] == iris.target[i] else 'y' for i, val in enumerate(prediction)]\n",
        "\n",
        "combinations = itertools.combinations(range(len(iris.feature_names)), 2)\n",
        "for index, combination in enumerate(combinations):\n",
        "    plt.subplot(2, 3, index+1)\n",
        "\n",
        "    labels = iris.feature_names[combination[0]], iris.feature_names[combination[1]]\n",
        "    plot_dataset(X[:, combination[0]], X[:, combination[1]], labels, colors)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "classification_iris_knn_aa_21_22.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4ee1453153d79d7ad1a82980d3c928237b76f76402eb5a7ca0ae977e8b6ccd3a"
    },
    "kernelspec": {
      "display_name": "Python [conda env:aaaid]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
