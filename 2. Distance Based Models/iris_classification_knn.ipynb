{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbORlHDOEYTE"
      },
      "source": [
        "## Classifiers comparison: decision trees and k-nearest neighbors on the dataset Iris\n",
        "\n",
        "\n",
        "In the following program we compare the prediction results obtained by decision trees and k-nearest neighbors on the dataset Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iFRSTXNDEYTG"
      },
      "source": [
        "The following cell shows the program training a decision tree and its results in preciction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7fYOeG3EYTH",
        "outputId": "94c06325-bd74-47a6-92b7-8e6158afddd2"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree \n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score # will be used to separate training and test\n",
        "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, auc\n",
        "import itertools\n",
        "iris = load_iris()\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})\n",
        "clf = clf.fit(iris.data, iris.target)\n",
        "scores = cross_val_score(clf, iris.data, iris.target, cv=5) # score will be the accuracy\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Stp1gpEYTP"
      },
      "source": [
        "The following cell shows the training of k-nearest neighbors and its prediction results.\n",
        "Here we use a uniform weighting setting (weights='uniform'): any neighbors weights the same in the majority voting aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZAavM5qEYTQ",
        "outputId": "75239f26-c991-4aa5-af30-4e1e11e60e34"
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors\n",
        "n_neighbors = 11\n",
        "clf_knn = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n",
        "clf_knn = clf_knn.fit(iris.data, iris.target)\n",
        "scores = cross_val_score(clf_knn, iris.data, iris.target, cv=5) # score will be the accuracy\n",
        "print(scores)\n",
        "# shows the model predictions  \n",
        "for i in range(len(iris.target)):\n",
        "    print(iris.data[i,:])\n",
        "    print(iris.data[i,:].reshape(1,-1))\n",
        "    instance=iris.data[i,:].reshape(1,-1)\n",
        "    #print(clf_knn.predict(instance))\n",
        "    predicted=clf_knn.predict(instance)[0]\n",
        "    print(predicted)\n",
        "    if iris.target[i]==predicted:\n",
        "        print(str(i)+\" ok \"+str(iris.target_names[iris.target[i]]))\n",
        "    else:\n",
        "        print(str(i)+\" nok \"+\"true class: \"+str(iris.target_names[iris.target[i]])+\"; predicted: \"+str(iris.target_names[predicted]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVI8abkeEYTV"
      },
      "source": [
        "In the following cell we use a varying weighting setting (weights='distance'): any neighbors weights inversely with its distance to the test instance in the majority voting aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxC1EypXEYTV",
        "outputId": "04a3ea00-6045-4507-e3c8-98bc124b4a5b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "n_neighbors = 11\n",
        "clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "clf_knn2.fit(iris.data, iris.target)\n",
        "\n",
        "for i in range(len(iris.target)):\n",
        "    instance=(iris.data[i,:]).reshape(1, -1)\n",
        "    predicted2=clf_knn2.predict(instance)[0]\n",
        "    if iris.target[i]==predicted2:\n",
        "        print(str(i)+\" ok \"+str(iris.target_names[iris.target[i]]))\n",
        "    else:\n",
        "        print(str(i)+\" nok \"+\"true class: \"+str(iris.target_names[iris.target[i]])+\"; predicted: \"+str(iris.target_names[predicted]))\n",
        "print(\"Classification score of k-nn with distance weighting\")\n",
        "scores2 = cross_val_score(clf_knn2, iris.data, iris.target, cv=5,scoring='accuracy') # score will be the accuracy\n",
        "print(scores2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wpx4NW0EYTa"
      },
      "source": [
        "The following cell shows the tuning of the k-nn models with a varying value of k (number of nearest neighbors) and finds the best value of k (giving the maximum accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-jhdpzK2EYTa",
        "outputId": "6eadc375-90cb-46bf-d6eb-237b942ff42f",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from sklearn import neighbors\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "best_accuracy=0\n",
        "best_k=1\n",
        "A=np.zeros(len(y_train), dtype=np.float) # for storing accuracies\n",
        "for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "    clf_knn3 = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "    # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "    clf_knn3.fit(X_train, y_train)\n",
        "    index=n_neighbors-1\n",
        "    A[index]=clf_knn3.score(X_test, y_test)\n",
        "    if best_accuracy<clf_knn3.score(X_test, y_test):\n",
        "        best_accuracy=clf_knn3.score(X_test, y_test)\n",
        "        best_k=n_neighbors\n",
        "    print(\"k neighbors=\"+str(n_neighbors))\n",
        "    print(\"accuracy=\"+str(clf_knn3.score(X_test, y_test)))\n",
        "    \n",
        "print(\"\\n\")\n",
        "print(\"best k=\"+str(best_k))\n",
        "print(\"best accuracy=\"+str(best_accuracy))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "#plt.xticks(np.arange(1, len(y_train)+1, 8))\n",
        "plt.yticks(np.arange(0.0,1.0,0.01))\n",
        "\n",
        "plt.plot(np.arange(1,len(y_train)+1),A)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "p23LUav0EYTe"
      },
      "source": [
        "In the following cell we plot in the same plot two subplots with the diagrams on accuracy with the two kinds of weighting \n",
        "of the vote of the neighbours (uniform and with distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ewb11oFOEYTf",
        "outputId": "a64d135d-b84c-4051-f1b6-5cd248b5c70f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from sklearn import neighbors\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "i=0  #parameter in the control of the subplot to draw on'\n",
        "f,(ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "for weight_type in ['uniform','distance']:\n",
        "    print(\"weighting:\"+str(weight_type))\n",
        "    A=np.zeros(len(y_train), dtype=np.float) # for storing accuracies\n",
        "    best_accuracy=0\n",
        "    best_k=1\n",
        "    for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "        clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights=weight_type)\n",
        "        # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "        clf_knn2.fit(X_train, y_train)\n",
        "        index=n_neighbors-1\n",
        "        A[index]=clf_knn2.score(X_test, y_test)\n",
        "        if best_accuracy<clf_knn2.score(X_test, y_test):\n",
        "            best_accuracy=clf_knn2.score(X_test, y_test)\n",
        "            best_k=n_neighbors\n",
        "        print(\"k neighbors=\"+str(n_neighbors))\n",
        "        print(\"accuracy=\"+str(clf_knn2.score(X_test, y_test)))\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(\"best k=\"+str(best_k))\n",
        "    print(\"best accuracy=\"+str(best_accuracy))\n",
        "    if i==0:\n",
        "        ax1.plot(np.arange(1,len(y_train)+1),A)\n",
        "        ax1.set_title('weighting type:'+str(weight_type))\n",
        "    else:\n",
        "        ax2.plot(np.arange(1,len(y_train)+1),A)\n",
        "        ax2.set_title('weighting type:'+str(weight_type))\n",
        "    i=i+1\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "LXhkXpz0EYTj"
      },
      "source": [
        "In the following cell we plot (overlapping) in the same picture both the diagrams on accuracy with the two kinds of weighting \n",
        "of the vote of the neighbours (uniform and with distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7_nkikvMEYTk",
        "outputId": "e5fb0243-2767-4448-e9b5-7252c8bf1610"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from sklearn import neighbors\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Accuracy in k-nn with number of neighbors and types of weighting', fontsize=14, fontweight='bold')\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel('n. neighbors')\n",
        "ax.set_ylabel('accuracy')\n",
        "\n",
        "A=np.zeros((len(y_train),2), dtype=np.float) # 2 arrays for storing accuracies for each type of weigthing\n",
        "i=0  #parameter in the control of the different diagram (=matrix A column index)\n",
        "best_accuracy=0\n",
        "for weight_type in ['uniform','distance']:\n",
        "    print(\"\\n weighting:\"+str(weight_type))\n",
        "    best_accuracy=0\n",
        "    best_k=1\n",
        "    for n_neighbors in np.arange(1,len(y_train)+1):\n",
        "        clf_knn2 = neighbors.KNeighborsClassifier(n_neighbors, weights=weight_type)\n",
        "        # (n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
        "        clf_knn2.fit(X_train, y_train)\n",
        "        index=n_neighbors-1 # computes the matrix row index\n",
        "        A[index,i]=clf_knn2.score(X_test, y_test)\n",
        "        if best_accuracy<clf_knn2.score(X_test, y_test):\n",
        "            best_accuracy=clf_knn2.score(X_test, y_test)\n",
        "            best_k=n_neighbors\n",
        "        print(\"k neighbors=\"+str(n_neighbors))\n",
        "        print(\"accuracy=\"+str(clf_knn2.score(X_test, y_test)))\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(\"best k=\"+str(best_k))\n",
        "    print(\"best accuracy=\"+str(best_accuracy))\n",
        "    plt.plot(np.arange(1,len(y_train)+1),A[:,i])\n",
        "    i=i+1\n",
        "plt.legend(['uniform', 'distance'], loc='lower left')  \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-aSX54vFEYTp"
      },
      "source": [
        "## 1) Plot the Iris dataset, in 2-D, with a red color for Setosa, blue for Versicolor, Green for Virginica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = iris.data\n",
        "y = iris.target\n",
        "palette = ['r', 'g', 'b']\n",
        "\n",
        "def plot_dataset(x, y, labels, colors):\n",
        "    plt.grid(True, alpha=0.4)\n",
        "    plt.xlabel(labels[0])\n",
        "    plt.ylabel(labels[1])\n",
        "    plt.scatter(x, y, c=colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o8cPLOdEYTq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 8))\n",
        "plt.suptitle(\"2-D Dataset Plots\")\n",
        "colors = [palette[val] for val in iris.target]\n",
        "\n",
        "combinations = itertools.combinations(range(len(iris.feature_names)), 2)\n",
        "for index, combination in enumerate(combinations):\n",
        "    plt.subplot(2, 3, index+1)\n",
        "\n",
        "    labels = iris.feature_names[combination[0]], iris.feature_names[combination[1]]\n",
        "    plot_dataset(X[:, combination[0]], X[:, combination[1]], labels, colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs9ogv4vEYTt"
      },
      "source": [
        "## 2) Plot the Iris dataset, in 2-D, with the color as above determined by the k-nn estimation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niu9K9kn43ZM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=300)\n",
        "n_neighbors = 7\n",
        "\n",
        "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
        "knn_classifier = knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "prediction = knn_classifier.predict(X)\n",
        "\n",
        "accuracy_score(iris.target, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 8))\n",
        "plt.suptitle(\"2-D Dataset Plots\")\n",
        "colors = [palette[val] for val in prediction]\n",
        "\n",
        "combinations = itertools.combinations(range(len(iris.feature_names)), 2)\n",
        "for index, combination in enumerate(combinations):\n",
        "    plt.subplot(2, 3, index+1)\n",
        "\n",
        "    labels = iris.feature_names[combination[0]], iris.feature_names[combination[1]]\n",
        "    plot_dataset(X[:, combination[0]], X[:, combination[1]], labels, colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Plot the ROC plot of the k-nn (for the best value of k) for each of the three classes: setosa, virginica and versicolor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subplot_kw = dict(\n",
        "        xlabel=('False Positive Rate'), \n",
        "        xlim=([-0.01, 1.0]), \n",
        "        ylabel=('True Positive Rate'), \n",
        "        ylim=([-0.01, 1.01]))\n",
        "\n",
        "def compute_rocs():\n",
        "    y_score = knn_classifier.predict_proba(X_test)\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    \n",
        "    # Compute ROC and AUC for each class\n",
        "    for i in range(3):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test, y_score[:,i], pos_label=i)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    return (fpr, tpr, roc_auc)\n",
        "\n",
        "def plot_rocs(title: str, fpr, tpr, roc_auc):\n",
        "    fig, (ax0, ax1, ax2) = plt.subplots(nrows=1, ncols=3, figsize=(25, 7), sharex=True, subplot_kw=subplot_kw)\n",
        "    plt.grid(True, alpha=0.4)\n",
        "\n",
        "    i = 0\n",
        "    ax0.set_title(iris.target_names[i])\n",
        "    ax0.plot(fpr[i], tpr[i], marker='o', markersize=4, color='blue', lw=2, label=f'AUC = {roc_auc[i]})')\n",
        "    ax0.plot([0, 1], [0, 1], marker='o', markersize=4, color='black', lw=1, linestyle='--')\n",
        "    ax0.legend(loc=\"lower right\")\n",
        "    ax0.grid(True, alpha=0.4)\n",
        "\n",
        "    i = 1\n",
        "    ax1.set_title(iris.target_names[i])\n",
        "    ax1.plot(fpr[i], tpr[i], marker='o', markersize=4, color='red', lw=2, label=f'AUC = {roc_auc[i]})')\n",
        "    ax1.plot([0, 1], [0, 1], marker='o', markersize=4, color='black', lw=1, linestyle='--')\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    ax1.grid(True, alpha=0.4)\n",
        "\n",
        "    i = 2\n",
        "    ax2.set_title(iris.target_names[i])\n",
        "    ax2.plot(fpr[i], tpr[i], marker='o', markersize=4, color='green', lw=2, label=f'AUC = {roc_auc[i]})')\n",
        "    ax2.plot([0, 1], [0, 1], marker='o', markersize=4, color='black', lw=1, linestyle='--')\n",
        "    ax2.legend(loc=\"lower right\")\n",
        "    ax2.grid(True, alpha=0.4)\n",
        "\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    return fig\n",
        "\n",
        "plot_rocs('ROC Plots', *compute_rocs()).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Plot the ROC plot for the best decision tree you trained in Exercize n.1  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6iglAFh37rQ"
      },
      "source": [
        "## 5) Compare decision trees and k-nn on the ROC space: for which values of (TPR,FPR) k-nn is better than decision trees?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4d3We4cEYTu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqQOXvLKEYTy"
      },
      "source": [
        "## 6) In the following, extend the above exercize on k-nn using a kernel function  \n",
        "## K(x,y) for the distances computation, such that distance(x,y)=1-K(x,y).    Use a Gaussian-like (or Radial Basis Function) kernel K(x,y)=exp(-gamma(x-y)^2), with gamma the inverse of the sigma squared variance, that must be tuned to the best value according to the accuracy reached by the k-nn, with k=7 (similarly as done with the previous example on tuning the parameter n_neightbors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWkBiriEYTy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "classification_iris_knn_aa_21_22.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4ee1453153d79d7ad1a82980d3c928237b76f76402eb5a7ca0ae977e8b6ccd3a"
    },
    "kernelspec": {
      "display_name": "Python [conda env:aaaid]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
