{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "random_state = 42\n",
    "min_samples_leaf = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get an artificial inflation of some class in the training set by a given factor: 10 (weigh more the classes virginica e versicolor which are more difficult to discriminate). Learn the tree in these conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, target = load_iris(return_X_y=True)\n",
    "iris = load_iris()\n",
    "inflation_factor = 10\n",
    "\n",
    "# Compute the repetitions of each elem of the dataset\n",
    "repetitions = np.clip(iris.target * inflation_factor, a_min=1, a_max=inflation_factor)\n",
    "\n",
    "# Apply inflation to dataset\n",
    "inflated_data = np.repeat(iris.data, repetitions, axis=0)\n",
    "inflated_target = np.repeat(iris.target, repetitions, axis=0)\n",
    "\n",
    "# Get train & test datasets\n",
    "X_train, X_test, y_train, y_test_inf = train_test_split(inflated_data, inflated_target, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Learning of the tree\n",
    "clf_inflated = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=random_state, min_samples_leaf=5)\n",
    "clf_inflated = clf_inflated.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_inf = clf_inflated.predict(X_test)\n",
    "\n",
    "# Show pairs\n",
    "pd.DataFrame(np.column_stack((y_pred_inf, y_test_inf)).T, index=['prediction', 'reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test_inf, y_pred_inf)\n",
    "report = classification_report(y_test_inf, y_pred_inf, labels=[0, 1, 2])\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100}%')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf_inflated, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Modify the weight of some classes (set to 10 the weights for misclassification between virginica into versicolor and vice versa) and learn the tree in these conditions. You should obtain similar results as for step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test_w = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "clf_weighted = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=21, min_samples_leaf=min_samples_leaf, class_weight={0:1,1:10,2:10})\n",
    "\n",
    "# Learning of the tree applying weights\n",
    "clf_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_w = clf_weighted.predict(X_test)\n",
    "\n",
    "# Show pairs\n",
    "pd.DataFrame(np.column_stack((y_pred_w, y_test)).T, index=['prediction', 'reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test_w, y_pred_w)\n",
    "report = classification_report(y_test_w, y_pred_w, labels=[0, 1, 2])\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100}%')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf_weighted, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Learn trees but avoid overfitting (by improving the error on the test set) tuning the parameters on: the minimum number of samples per leaf, max depth of the tree, min_impurity_decrease parameters, max leaf nodes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "hyperparameters = list()\n",
    "\n",
    "for min_samples_l in range(2, 15):\n",
    "    for max_leaf_n in range(2, 15):\n",
    "        for min_samples_s in range(2, 30):\n",
    "            for impurity in np.linspace(0, 1, 21):\n",
    "                clf_improved = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=random_state, min_samples_leaf=min_samples_l, max_leaf_nodes=max_leaf_n, min_samples_split=min_samples_s, min_impurity_decrease=impurity)\n",
    "                clf_improved = clf_improved.fit(X_train, y_train)\n",
    "                y_pred_imp = clf_improved.predict(X_test)\n",
    "\n",
    "                accuracy = accuracy_score(y_test, y_pred_imp)\n",
    "                f1 = f1_score(y_test, y_pred_imp, average='macro')\n",
    "                if accuracy + f1 == 2:\n",
    "                    hyperparameters.append([min_samples_l, max_leaf_n, min_samples_s, impurity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_improved = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=random_state, min_samples_leaf=hyperparameters[0][0], max_leaf_nodes=hyperparameters[0][1], min_samples_split=hyperparameters[0][2], min_impurity_decrease=hyperparameters[0][3])\n",
    "clf_improved = clf_improved.fit(X_train, y_train)\n",
    "y_pred_imp = clf_improved.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array(hyperparameters)\n",
    "df = pd.DataFrame(mat, columns=[\"min_samples_leaf\", \"max_leaf_nodes\", \"min_samples_split\", \"min_impurity_decrease\"])\n",
    "df.sort_values([\"min_samples_leaf\", \"max_leaf_nodes\", \"min_samples_split\", \"min_impurity_decrease\"], inplace=True)\n",
    "df.to_html(\"optimized_hyperparameters.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the table created with the optimized hyperparameters we see that:\n",
    "* <b>min_samples_leaf</b>: from 2 to 8\n",
    "* <b>max_leaf_nodes</b>: from 4 to 14\n",
    "* <b>min_samples_split</b>: from 2 to 29\n",
    "* <b>min_impurity_decrease</b>: 0 to 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Build the confusion matrix of the created tree models on the test set and show them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflated_confusion_matrix = confusion_matrix(y_true=y_test_inf, y_pred=y_pred_inf)\n",
    "weighted_confusion_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred_w)\n",
    "improved_confusion_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred_imp)\n",
    "\n",
    "print(pd.DataFrame(inflated_confusion_matrix, index=iris.target_names, columns=iris.target_names))\n",
    "print(pd.DataFrame(weighted_confusion_matrix, index=iris.target_names, columns=iris.target_names))\n",
    "print(pd.DataFrame(improved_confusion_matrix, index=iris.target_names, columns=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Build the ROC curves (or coverage curves in coverage space) and plot them for each tree model you have created: for each model you have to build three curves, one for each class, considered in turn as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c45cb57a8e615f538fcf6c9f1cc2590ada0a8328b3cfcbe1c20f2dcc2b08d1fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('pyVenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
